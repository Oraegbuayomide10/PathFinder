{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the DeepGlobe dataset (6,226 images) I reserve a 500 images for validation - 500\n",
    "# for the Spacenet dataset (2,780 images) I reserve a 500 images for validation - 500\n",
    "# for the WHU dataset (27,770 images)  I reserve a 500 images for validation - 1000\n",
    "\n",
    "#\n",
    "\n",
    "# update the number of images processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from PATHFinder.utils.download_weights import Download_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variable is set!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Propietario\\miniconda3\\envs\\datascience\\lib\\site-packages\\paramiko\\pkey.py:82: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "c:\\Users\\Propietario\\miniconda3\\envs\\datascience\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.Blowfish and will be removed from this module in 45.0.0.\n",
      "  \"class\": algorithms.Blowfish,\n",
      "c:\\Users\\Propietario\\miniconda3\\envs\\datascience\\lib\\site-packages\\paramiko\\transport.py:243: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You set no_training parameter, so training won't occur",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_val_model_multifolders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Train_validate_model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m validation_loss \u001b[38;5;241m=\u001b[39m \u001b[43mTrain_validate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_log_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWORKS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMaster_Thesis\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCodes\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mlogs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcls_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfreeze_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mphi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msave_period\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTesting\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43muse_f16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfreeze_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43munfreeze_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtotal_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43munfreeze_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minitial_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use 1e-4, you used 5e-5 for the last training\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# lad = 0.4,\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mplot_train_val_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mimages_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWORKS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMaster_Thesis\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTrain_Datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlabels_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWORKS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMaster_Thesis\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mTrain_Datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mearly_stopping_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mno_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\WORKS\\Master_Thesis\\Codes\\PathFinder\\seg_road\\train_val_model_multifolders.py:172\u001b[0m, in \u001b[0;36mTrain_validate_model\u001b[1;34m(images_directory, labels_directory, device, use_f16, num_classes, phi, pretrained, image_input_shape, train_percent, unfreeze_epoch, total_epochs, freeze_batch_size, unfreeze_batch_size, freeze_train, initial_lr, optimizer_type, momentum, weight_decay, lr_decay_type, save_period, save_log_dir, dice_loss, focal_loss, cls_weights, model_name, save_best_model, plot_train_val_loss, lad, c3_weight, c1_weight, early_stopping_epochs, no_training)\u001b[0m\n\u001b[0;32m    168\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m item)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_training:\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set no_training parameter, so training won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt occur\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m#------------- message on number of training epochs to ensure enough training\u001b[39;00m\n\u001b[0;32m    175\u001b[0m recommended_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5e-4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimizer_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madamW\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5e-4\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: You set no_training parameter, so training won't occur"
     ]
    }
   ],
   "source": [
    "from train import Train\n",
    "import numpy as np\n",
    "validation_loss = Train(\n",
    "                    pretrained=True,\n",
    "                    device='cuda',\n",
    "                    save_log_dir=r'C:\\WORKS\\Master_Thesis\\Codes\\logs',\n",
    "                    cls_weights=np.array([1, 3], dtype=np.float32),\n",
    "                    freeze_train=True,\n",
    "                    phi='b5',\n",
    "                    save_period=1,\n",
    "                    model_name=f'Testing',\n",
    "                    use_f16=False, \n",
    "                    freeze_batch_size=16,\n",
    "                    unfreeze_batch_size=8,\n",
    "                    total_epochs=100,\n",
    "                    unfreeze_epoch=50,\n",
    "                    initial_lr= 1e-4, # use 1e-4, you used 5e-5 for the last training\n",
    "                    # lad = 0.4,\n",
    "                    plot_train_val_loss=True,\n",
    "                    images_directory=r\"C:\\WORKS\\Master_Thesis\\Train_Datasets\",\n",
    "                    labels_directory=r\"C:\\WORKS\\Master_Thesis\\Train_Datasets\",\n",
    "                    early_stopping_epochs=5,\n",
    "                    no_training=True\n",
    "                    )\n",
    "\n",
    "\n",
    "# Validation Loss: 0.6311, MIoU: 0.6431 - author's lad\n",
    "\n",
    "\n",
    "\n",
    "#---- for the b5 all spacenet, WHU and deepglobe dataset\n",
    "#Training Loss: 0.5609,  MIoU: 0.7488, LR: 4.9924335535884686e-05                                                             \n",
    "# Validation Loss: 0.5828, MIoU: 0.7437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  - still trying to balance it = Validation Loss: 0.5602, MIoU: 0.5471\n",
    "#- better one = Validation Loss: 0.5471, MIoU: 0.6198\n",
    "\n",
    "\n",
    "#Validation Loss: 0.6049, MIoU: 0.7053\n",
    "#0.26000  - initial width of the road\n",
    "#-0.000012 = 1.34 meters\n",
    "\n",
    "# Validation Loss: 0.5445, MIoU: 0.6335 - best UN unfroozen finetune\n",
    "#https://www.youtube.com/watch?v=bNNfUhzivxk - pastor nathaneil's ministration at prayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "new_image = Image.new('L', [512, 512], (0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273731_sat\n",
      "26072_sat\n",
      "13524_sat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "some_path = ['C:\\\\WORKS\\\\Master_Thesis\\\\Train_Datasets\\\\DeepGlobe\\\\273731_sat.jpg', 'C:\\\\WORKS\\\\Master_Thesis\\\\Train_Datasets\\\\WHU\\\\26072_sat.jpg', 'C:\\\\WORKS\\\\Master_Thesis\\\\Train_Datasets\\\\WHU\\\\13524_sat.jpg']\n",
    "file_names = [os.path.splitext(os.path.basename(path))[0] for path in some_path]\n",
    "for file in file_names:\n",
    "    if 'sat' in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Things used while training\n",
    "\n",
    "1. fp16 - using mixed precision for faster training to save GPU\n",
    "\n",
    "2. num_classes is 2 - background and foreground\n",
    "\n",
    "3. phi - which defines the backbone model used. In the code, phi b2 was used\n",
    "4. pretrained - whether to use the segformer pretrained weights. In the code it was set to false, I do know why yet but I am going to check. In the code if pretrained is set to false, then the weights were intialised using something called xavier .... check the train.py file for more info\n",
    "5. Image size - the image size used in the code is 512 by 512. but it seems like it can be changed and the best possible image sizes to use were defined in the segformer.py file \n",
    "6. frozen and unfrozen phase of training - this has to do with how many epochs to train with the backbone frozen. the batch size and epochs for freezing and unfreezing are specified. it was also specified in the paper. so try and check if paper and code correlates. Inital learning rate, momentum,weight decay, batch size and epochs were specified for frozen and unfroozen stages of training  \n",
    "7. learning_rate_decay - cosine lr decay was used\n",
    "8. optimizer  - AdamW was used in the code, Adam was used in the paper\n",
    "9. save_period - how often per epochs to save the weight of the model\n",
    "10. eval_flag and eval_period - how often to perform evaluation during training. This might not be necessary.\n",
    "11. Dice loss and Focal loss - both were set to False, meaning both scores were not used as loss functions. This correlates with what is in the paper as binary cross entropy loss and cross entropy loss were used in the paper. Cross entropy loss was used as the segmentation loss while binary cross entropy loss was used as PCS loss\n",
    "12. num_workers - the value 4 was used. But most times when I set num_workers my code doesn't run. So I will need to check why.\n",
    "13. Training for multiple GPU, single GPU or CPU is specified\n",
    "14. Pretrained_weights - downloading of pretrained weights if petrained is set to True. Otherwise if pretrained is set to False, it applies weight initialisation to the model. The function to define weights is in the repo.\n",
    "15. Mixed_precision - if f16 is specified, the code uses Gradscaler to scale gradients, this helps with numerical stability. If f16 is set to False i.e not used, the scaler is set to None\n",
    "16. Synchronised Batch Normalisation - this can only be used when you have multiGPUS\n",
    "17. Distributed Mode - this is used only on multiple GPUs\n",
    "18. A configuration display function which displays key configuration parameters like lr, num_classes, model, input_shape\n",
    "19. In the code, a warning is set to make sure the model is trained for enough epochs especially when using optimizers like the AdamW as this type of optimizer requires longer training steps to reach convergence. So a recommend steps of about 15000 is set if an Adamw optimizer is used else if another type of optimizer the recommended steps is set to about 5000. The total number of steps the model will perform for the whole training of the model is defined as the length of the whole training data // unfreeze batch_size multiply by the unfreeze epoch *In code - wanted_step = 1.5e4 if optimizer_type == \"adamw\" else 0.5e4\n",
    "total_step = num_train // Unfreeze_batch_size * UnFreeze_Epoch*.\n",
    "20. When training the model, if freezing backbone layers is set to True, then the updating of the model's layer weights during backpropagation is set to False. This enables the pre-trained weights of the model to be retained,while training only the top layers of the model. This was done by setting requires grad to False in the train.py file -line 395\n",
    "21. In line 406 to 410 a new minimum and maximum learning rate is defined for a batchsize different from the reference batchsize of 16. The reference batchsize is 16, but of course it is assumed that there might be difference s in computation constraints, so a new minimum and maximum learning rate is calculated using the reference batch size of 16. The code is from 406 to 410. The idea is that higher batch size requires larger learning rate (e.g 0.16) since there would be more samples seen by the model and when calculating gradients, it won't be small. Also, smaller batch sizes requires smaller learning rate (e.g 0.0025). The idea is to scale the learning rate proportionally to the batch size relative to a reference batch size. The max and min learning rate for the reference batch size of 16 is also specified.\n",
    "22. The model is trained from an init epoch to the unfreeze epoch. This is defined as a range. The batch size used during training when the layers are freezed and when the layers are unfreezed are different. But  it is recommended that the Freeze_batch_size be 1-2 times the size of the Unfreeze_batch_size. The difference between both batch sizes shouldn't be much. In the code, default freeze batch size is set to 16, and default unfreeze batch size is set to 8. The batch size when the layers are unfrozen is smaller because it would require more gpu computation and memory to train since all the layers are unfrozen.\n",
    "23. So the code was firstly designed for freezing the layers, then after running for the defined freeze epoch, the layers are then unfrozen so that the unfroozen epoch can be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.2/' with OidcBearerAuth>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from shapely.wkt import loads\n",
    "import openeo\n",
    "from tqdm import tqdm\n",
    "connection = openeo.connect(\"openeo.dataspace.copernicus.eu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Connection to 'https://openeo.dataspace.copernicus.eu/openeo/1.2/' with OidcBearerAuth>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection.authenticate_oidc_client_credentials(\n",
    "                             client_id='sh-46deba5e-c869-445d-acf7-9e1a0fc067fe',\n",
    "                              client_secret='A2VIFN38cGykGEWRklbNDiY8McvY86kB'\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
